{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lukemiller1987/cs5565-assign-2?scriptVersionId=135629675\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"4c1b653c","metadata":{"_execution_state":"idle","_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","execution":{"iopub.execute_input":"2023-07-03T18:48:10.958769Z","iopub.status.busy":"2023-07-03T18:48:10.957064Z","iopub.status.idle":"2023-07-03T18:48:12.076737Z","shell.execute_reply":"2023-07-03T18:48:12.074766Z"},"papermill":{"duration":1.135644,"end_time":"2023-07-03T18:48:12.079169","exception":false,"start_time":"2023-07-03T18:48:10.943525","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n","\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.2     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n","\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n","\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.4.2     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n","\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n","\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.1     \n","── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n","\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n","\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n","\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n"]},{"data":{"text/html":["'cs5565-datasets'"],"text/latex":["'cs5565-datasets'"],"text/markdown":["'cs5565-datasets'"],"text/plain":["[1] \"cs5565-datasets\""]},"metadata":{},"output_type":"display_data"}],"source":["# This R environment comes with many helpful analytics packages installed\n","# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n","# For example, here's a helpful package to load\n","\n","library(tidyverse) # metapackage of all tidyverse packages\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","list.files(path = \"../input\")\n","Boston = read.csv(\"/kaggle/input/cs5565-datasets/ALL CSV FILES - 2nd Edition/Boston.csv\")\n","Wages = read.csv(\"/kaggle/input/cs5565-datasets/ALL CSV FILES - 2nd Edition/Wage.csv\")\n","College = read.csv(\"/kaggle/input/cs5565-datasets/ALL CSV FILES - 2nd Edition/College.csv\")\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"d89a2f69","metadata":{"papermill":{"duration":0.010452,"end_time":"2023-07-03T18:48:12.100805","exception":false,"start_time":"2023-07-03T18:48:12.090353","status":"completed"},"tags":[]},"source":["Chapter 6 Conceptual:\n","ISLR 6.6 Problem 1 (a)-(c) (page 282) +3 points\n","ISLR 6.6 Problem 2 (a)-(c) (page 283) +3 points\n","\n","Chapter 6 Lab:\n","ISLR 6.6 Applied Problem 8 (a)-(f) (page 285) +6 points\n","ISLR 6.6 Applied Problem 9 (a)-(d) (page 285) +4 points\n","         (do not do (e), (f), or (g) on problem 9)\n","\n","(See link for assistance with Conceptual & Lab Problems: https://rpubs.com/lmorgan95/ISLR_CH6_SolutionsLinks to an external site.) \n","\n","Chapter 7 Conceptual:\n","ISLR 7.9 Problem 4 (page 323) +3 points\n","ISLR 7.9 Problem 5 (a)-(c) (page 323) +3 points\n","\n","Chapter 7 Lab:\n","ISLR 7.9 Applied Problem 6 (a)-(b) (page 323) +2 points\n","ISLR 7.9 Applied Problem 9 (a)-(f) (page 324) +6 points\n","\n","\n","(See link for assistance with Conceptual & Lab Problems: https://rpubs.com/lmorgan95/ISLR_CH7_SolutionsLinks to an external site.) "]},{"cell_type":"markdown","id":"9315b307","metadata":{"papermill":{"duration":0.010379,"end_time":"2023-07-03T18:48:12.121464","exception":false,"start_time":"2023-07-03T18:48:12.111085","status":"completed"},"tags":[]},"source":["# 6.6 Exercises\n","## Conceptual\n","### 1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2,...,p predictors. Explain your answers:\n","#### (a) Which of the three models with k predictors has the smallest training RSS?"]},{"cell_type":"markdown","id":"9c7def51","metadata":{"papermill":{"duration":0.010333,"end_time":"2023-07-03T18:48:12.142272","exception":false,"start_time":"2023-07-03T18:48:12.131939","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"6723621d","metadata":{"papermill":{"duration":0.010362,"end_time":"2023-07-03T18:48:12.1636","exception":false,"start_time":"2023-07-03T18:48:12.153238","status":"completed"},"tags":[]},"source":["#### (b) Which of the three models with k predictors has the smallest test RSS?\n"]},{"cell_type":"markdown","id":"d075580b","metadata":{"papermill":{"duration":0.010451,"end_time":"2023-07-03T18:48:12.184353","exception":false,"start_time":"2023-07-03T18:48:12.173902","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"f554a6d7","metadata":{"papermill":{"duration":0.010167,"end_time":"2023-07-03T18:48:12.205087","exception":false,"start_time":"2023-07-03T18:48:12.19492","status":"completed"},"tags":[]},"source":["#### (c) True or False:\n","##### i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection."]},{"cell_type":"markdown","id":"17dc141e","metadata":{"papermill":{"duration":0.010811,"end_time":"2023-07-03T18:48:12.22665","exception":false,"start_time":"2023-07-03T18:48:12.215839","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"23d01c5b","metadata":{"papermill":{"duration":0.01048,"end_time":"2023-07-03T18:48:12.247483","exception":false,"start_time":"2023-07-03T18:48:12.237003","status":"completed"},"tags":[]},"source":["##### ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by backward stepwise selection."]},{"cell_type":"markdown","id":"d4aa96e7","metadata":{"papermill":{"duration":0.010285,"end_time":"2023-07-03T18:48:12.268105","exception":false,"start_time":"2023-07-03T18:48:12.25782","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"6227fc88","metadata":{"papermill":{"duration":0.01038,"end_time":"2023-07-03T18:48:12.288656","exception":false,"start_time":"2023-07-03T18:48:12.278276","status":"completed"},"tags":[]},"source":["##### iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by forward stepwise selection."]},{"cell_type":"markdown","id":"15f3b86c","metadata":{"papermill":{"duration":0.010176,"end_time":"2023-07-03T18:48:12.309123","exception":false,"start_time":"2023-07-03T18:48:12.298947","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"e3a04a71","metadata":{"papermill":{"duration":0.010198,"end_time":"2023-07-03T18:48:12.329743","exception":false,"start_time":"2023-07-03T18:48:12.319545","status":"completed"},"tags":[]},"source":["##### iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection."]},{"cell_type":"markdown","id":"f011a3c2","metadata":{"papermill":{"duration":0.010207,"end_time":"2023-07-03T18:48:12.350191","exception":false,"start_time":"2023-07-03T18:48:12.339984","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"81ad5400","metadata":{"papermill":{"duration":0.010179,"end_time":"2023-07-03T18:48:12.370755","exception":false,"start_time":"2023-07-03T18:48:12.360576","status":"completed"},"tags":[]},"source":["##### v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection."]},{"cell_type":"markdown","id":"7478cc29","metadata":{"papermill":{"duration":0.010166,"end_time":"2023-07-03T18:48:12.391226","exception":false,"start_time":"2023-07-03T18:48:12.38106","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"df99875e","metadata":{"papermill":{"duration":0.010001,"end_time":"2023-07-03T18:48:12.411367","exception":false,"start_time":"2023-07-03T18:48:12.401366","status":"completed"},"tags":[]},"source":["### 2. For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer.\n","#### (a) The lasso, relative to least squares, is:\n","##### i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease invariance.\n","##### ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n","##### iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n","##### iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias"]},{"cell_type":"markdown","id":"6f1e527c","metadata":{"papermill":{"duration":0.010121,"end_time":"2023-07-03T18:48:12.43166","exception":false,"start_time":"2023-07-03T18:48:12.421539","status":"completed"},"tags":[]},"source":["# Fix me"]},{"cell_type":"markdown","id":"d8b31dae","metadata":{"papermill":{"duration":0.010314,"end_time":"2023-07-03T18:48:12.452104","exception":false,"start_time":"2023-07-03T18:48:12.44179","status":"completed"},"tags":[]},"source":["#### (b) Repeat (a) for ridge regression relative to least squares.\n","##### i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease invariance.\n","##### ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n","##### iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n","##### iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias"]},{"cell_type":"markdown","id":"ba378563","metadata":{"papermill":{"duration":0.010079,"end_time":"2023-07-03T18:48:12.472497","exception":false,"start_time":"2023-07-03T18:48:12.462418","status":"completed"},"tags":[]},"source":["# Fix me"]},{"cell_type":"markdown","id":"23b09efd","metadata":{"papermill":{"duration":0.01034,"end_time":"2023-07-03T18:48:12.493119","exception":false,"start_time":"2023-07-03T18:48:12.482779","status":"completed"},"tags":[]},"source":["#### (c) Repeat (a) for non-linear methods relative to least squares.\n","##### i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease invariance.\n","##### ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n","##### iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n","##### iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias"]},{"cell_type":"markdown","id":"b5ce31c0","metadata":{"papermill":{"duration":0.010384,"end_time":"2023-07-03T18:48:12.514027","exception":false,"start_time":"2023-07-03T18:48:12.503643","status":"completed"},"tags":[]},"source":["# Fix me"]},{"cell_type":"markdown","id":"3d74e677","metadata":{"papermill":{"duration":0.010311,"end_time":"2023-07-03T18:48:12.53463","exception":false,"start_time":"2023-07-03T18:48:12.524319","status":"completed"},"tags":[]},"source":["## Applied\n","### 8. In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\n","#### (a) Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector ϵ of length n = 100."]},{"cell_type":"code","execution_count":2,"id":"64b108b2","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:12.581689Z","iopub.status.busy":"2023-07-03T18:48:12.557568Z","iopub.status.idle":"2023-07-03T18:48:12.590946Z","shell.execute_reply":"2023-07-03T18:48:12.589565Z"},"papermill":{"duration":0.048299,"end_time":"2023-07-03T18:48:12.593302","exception":false,"start_time":"2023-07-03T18:48:12.545003","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"14a69f42","metadata":{"papermill":{"duration":0.010536,"end_time":"2023-07-03T18:48:12.646608","exception":false,"start_time":"2023-07-03T18:48:12.636072","status":"completed"},"tags":[]},"source":["#### (b) Generate a response vector Y of length n = 100 according to the model Y = β0 + β1X + β2X2 + β3X3 + ϵ, where β0, β1, β2, and β3 are constants of your choice."]},{"cell_type":"code","execution_count":3,"id":"0d83a586","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:12.671155Z","iopub.status.busy":"2023-07-03T18:48:12.669909Z","iopub.status.idle":"2023-07-03T18:48:12.679553Z","shell.execute_reply":"2023-07-03T18:48:12.678264Z"},"papermill":{"duration":0.024142,"end_time":"2023-07-03T18:48:12.681355","exception":false,"start_time":"2023-07-03T18:48:12.657213","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"cb469c59","metadata":{"papermill":{"duration":0.010192,"end_time":"2023-07-03T18:48:12.702029","exception":false,"start_time":"2023-07-03T18:48:12.691837","status":"completed"},"tags":[]},"source":["#### (c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X, X2,...,X10. What is the best model obtained according to Cp, BIC, and adjusted R2? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y ."]},{"cell_type":"code","execution_count":4,"id":"0a46f0b9","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:12.725604Z","iopub.status.busy":"2023-07-03T18:48:12.724379Z","iopub.status.idle":"2023-07-03T18:48:12.734997Z","shell.execute_reply":"2023-07-03T18:48:12.733653Z"},"papermill":{"duration":0.032732,"end_time":"2023-07-03T18:48:12.745267","exception":false,"start_time":"2023-07-03T18:48:12.712535","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"d9f02217","metadata":{"papermill":{"duration":0.01063,"end_time":"2023-07-03T18:48:12.766694","exception":false,"start_time":"2023-07-03T18:48:12.756064","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"b18ce893","metadata":{"papermill":{"duration":0.011302,"end_time":"2023-07-03T18:48:12.788245","exception":false,"start_time":"2023-07-03T18:48:12.776943","status":"completed"},"tags":[]},"source":["#### (d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?"]},{"cell_type":"code","execution_count":5,"id":"12aa8676","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:12.812847Z","iopub.status.busy":"2023-07-03T18:48:12.81146Z","iopub.status.idle":"2023-07-03T18:48:12.821985Z","shell.execute_reply":"2023-07-03T18:48:12.820625Z"},"papermill":{"duration":0.025449,"end_time":"2023-07-03T18:48:12.824199","exception":false,"start_time":"2023-07-03T18:48:12.79875","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"df44bff4","metadata":{"papermill":{"duration":0.01035,"end_time":"2023-07-03T18:48:12.845241","exception":false,"start_time":"2023-07-03T18:48:12.834891","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"bd33d90f","metadata":{"papermill":{"duration":0.010274,"end_time":"2023-07-03T18:48:12.865857","exception":false,"start_time":"2023-07-03T18:48:12.855583","status":"completed"},"tags":[]},"source":["#### (e) Now fit a lasso model to the simulated data, again using X, X2, ...,X10 as predictors. Use cross-validation to select the optimal value of λ. Create plots of the cross-validation error as a function of λ. Report the resulting coefficient estimates, and discuss the results obtained."]},{"cell_type":"code","execution_count":6,"id":"3d975080","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:12.889676Z","iopub.status.busy":"2023-07-03T18:48:12.888272Z","iopub.status.idle":"2023-07-03T18:48:12.898386Z","shell.execute_reply":"2023-07-03T18:48:12.89703Z"},"papermill":{"duration":0.024396,"end_time":"2023-07-03T18:48:12.900532","exception":false,"start_time":"2023-07-03T18:48:12.876136","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"4d126555","metadata":{"papermill":{"duration":0.010016,"end_time":"2023-07-03T18:48:12.920882","exception":false,"start_time":"2023-07-03T18:48:12.910866","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"8373f331","metadata":{"papermill":{"duration":0.010229,"end_time":"2023-07-03T18:48:12.941249","exception":false,"start_time":"2023-07-03T18:48:12.93102","status":"completed"},"tags":[]},"source":["#### (f) Now generate a response vector Y according to the model Y = β0 + β7X7 + ϵ, and perform best subset selection and the lasso. Discuss the results obtained."]},{"cell_type":"code","execution_count":7,"id":"6e6989b1","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:12.964786Z","iopub.status.busy":"2023-07-03T18:48:12.963483Z","iopub.status.idle":"2023-07-03T18:48:12.973544Z","shell.execute_reply":"2023-07-03T18:48:12.972111Z"},"papermill":{"duration":0.024169,"end_time":"2023-07-03T18:48:12.975741","exception":false,"start_time":"2023-07-03T18:48:12.951572","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"79bb9196","metadata":{"papermill":{"duration":0.01028,"end_time":"2023-07-03T18:48:12.996567","exception":false,"start_time":"2023-07-03T18:48:12.986287","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"ecbff2af","metadata":{"papermill":{"duration":0.010924,"end_time":"2023-07-03T18:48:13.019022","exception":false,"start_time":"2023-07-03T18:48:13.008098","status":"completed"},"tags":[]},"source":["### 9. In this exercise, we will predict the number of applications received using the other variables in the College data set.\n","#### (a) Split the data set into a training set and a test set."]},{"cell_type":"code","execution_count":8,"id":"f30edb6a","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:13.04448Z","iopub.status.busy":"2023-07-03T18:48:13.042746Z","iopub.status.idle":"2023-07-03T18:48:13.054187Z","shell.execute_reply":"2023-07-03T18:48:13.052775Z"},"papermill":{"duration":0.026712,"end_time":"2023-07-03T18:48:13.056424","exception":false,"start_time":"2023-07-03T18:48:13.029712","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"93256806","metadata":{"papermill":{"duration":0.010941,"end_time":"2023-07-03T18:48:13.078234","exception":false,"start_time":"2023-07-03T18:48:13.067293","status":"completed"},"tags":[]},"source":["#### (b) Fit a linear model using least squares on the training set, and report the test error obtained."]},{"cell_type":"code","execution_count":9,"id":"867b555a","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:13.104045Z","iopub.status.busy":"2023-07-03T18:48:13.102741Z","iopub.status.idle":"2023-07-03T18:48:13.11298Z","shell.execute_reply":"2023-07-03T18:48:13.111559Z"},"papermill":{"duration":0.02632,"end_time":"2023-07-03T18:48:13.115383","exception":false,"start_time":"2023-07-03T18:48:13.089063","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"604975b8","metadata":{"papermill":{"duration":0.010977,"end_time":"2023-07-03T18:48:13.138363","exception":false,"start_time":"2023-07-03T18:48:13.127386","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"5e1f4317","metadata":{"papermill":{"duration":0.011246,"end_time":"2023-07-03T18:48:13.160835","exception":false,"start_time":"2023-07-03T18:48:13.149589","status":"completed"},"tags":[]},"source":["#### (c) Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained."]},{"cell_type":"code","execution_count":10,"id":"f158e112","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:13.185736Z","iopub.status.busy":"2023-07-03T18:48:13.184509Z","iopub.status.idle":"2023-07-03T18:48:13.193601Z","shell.execute_reply":"2023-07-03T18:48:13.19239Z"},"papermill":{"duration":0.023645,"end_time":"2023-07-03T18:48:13.195539","exception":false,"start_time":"2023-07-03T18:48:13.171894","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"a7f52c93","metadata":{"papermill":{"duration":0.011123,"end_time":"2023-07-03T18:48:13.217832","exception":false,"start_time":"2023-07-03T18:48:13.206709","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"5b3e158b","metadata":{"papermill":{"duration":0.010967,"end_time":"2023-07-03T18:48:13.239662","exception":false,"start_time":"2023-07-03T18:48:13.228695","status":"completed"},"tags":[]},"source":["#### (d) Fit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates."]},{"cell_type":"code","execution_count":11,"id":"47f8df49","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:13.264494Z","iopub.status.busy":"2023-07-03T18:48:13.26311Z","iopub.status.idle":"2023-07-03T18:48:13.273879Z","shell.execute_reply":"2023-07-03T18:48:13.272463Z"},"papermill":{"duration":0.025792,"end_time":"2023-07-03T18:48:13.27629","exception":false,"start_time":"2023-07-03T18:48:13.250498","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"5fd9abab","metadata":{"papermill":{"duration":0.010941,"end_time":"2023-07-03T18:48:13.298496","exception":false,"start_time":"2023-07-03T18:48:13.287555","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"f4549c1a","metadata":{"papermill":{"duration":0.011078,"end_time":"2023-07-03T18:48:13.320737","exception":false,"start_time":"2023-07-03T18:48:13.309659","status":"completed"},"tags":[]},"source":["# 7.9 Exercises\n","## Conceptual\n","### 4. Suppose we fit a curve with basis functions b1(X) = I(0 ≤ X ≤ 2) − (X −1)I(1 ≤ X ≤ 2), b2(X)=(X −3)I(3 ≤ X ≤ 4) +I(4 < X ≤ 5). We fit the linear regression model Y = β0 + β1b1(X) + β2b2(X) + ϵ, and obtain coefficient estimates βˆ0 = 1, βˆ1 = 1, βˆ2 = 3. Sketch the estimated curve between X = −2 and X = 6. Note the intercepts, slopes, and other relevant information."]},{"cell_type":"code","execution_count":12,"id":"70da87e3","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:13.347212Z","iopub.status.busy":"2023-07-03T18:48:13.345811Z","iopub.status.idle":"2023-07-03T18:48:13.35662Z","shell.execute_reply":"2023-07-03T18:48:13.355227Z"},"papermill":{"duration":0.026317,"end_time":"2023-07-03T18:48:13.358928","exception":false,"start_time":"2023-07-03T18:48:13.332611","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"b9ccd658","metadata":{"papermill":{"duration":0.010891,"end_time":"2023-07-03T18:48:13.381142","exception":false,"start_time":"2023-07-03T18:48:13.370251","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"e990230b","metadata":{"papermill":{"duration":0.010853,"end_time":"2023-07-03T18:48:13.403056","exception":false,"start_time":"2023-07-03T18:48:13.392203","status":"completed"},"tags":[]},"source":["# ### 5. Consider two curves, ˆg1 and ˆg2, defined by\n","![](https://i.imgur.com/kdHWYGH.png)\n","\n","where g(m) represents the mth derivative of g.\n","#### (a) As λ → ∞, will ˆg1 or ˆg2 have the smaller training RSS?"]},{"cell_type":"markdown","id":"1cfa1a4e","metadata":{"papermill":{"duration":0.010715,"end_time":"2023-07-03T18:48:13.424497","exception":false,"start_time":"2023-07-03T18:48:13.413782","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"0f8da37d","metadata":{"papermill":{"duration":0.011077,"end_time":"2023-07-03T18:48:13.447426","exception":false,"start_time":"2023-07-03T18:48:13.436349","status":"completed"},"tags":[]},"source":["#### (b) As λ → ∞, will ˆg1 or ˆg2 have the smaller test RSS?"]},{"cell_type":"markdown","id":"0aad7dce","metadata":{"papermill":{"duration":0.010598,"end_time":"2023-07-03T18:48:13.468913","exception":false,"start_time":"2023-07-03T18:48:13.458315","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"38ea8186","metadata":{"papermill":{"duration":0.010523,"end_time":"2023-07-03T18:48:13.490043","exception":false,"start_time":"2023-07-03T18:48:13.47952","status":"completed"},"tags":[]},"source":["#### (c) For λ = 0, will ˆg1 or ˆg2 have the smaller training and test RSS?"]},{"cell_type":"markdown","id":"b9ef4739","metadata":{"papermill":{"duration":0.010716,"end_time":"2023-07-03T18:48:13.511896","exception":false,"start_time":"2023-07-03T18:48:13.50118","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"1f842bb6","metadata":{"papermill":{"duration":0.01057,"end_time":"2023-07-03T18:48:13.533107","exception":false,"start_time":"2023-07-03T18:48:13.522537","status":"completed"},"tags":[]},"source":["## Applied\n","### 6. In this exercise, you will further analyze the Wage data set considered throughout this chapter.\n","#### (a) Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data"]},{"cell_type":"markdown","id":"e12882f6","metadata":{"papermill":{"duration":0.01059,"end_time":"2023-07-03T18:48:13.554498","exception":false,"start_time":"2023-07-03T18:48:13.543908","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"8b9566d4","metadata":{"papermill":{"duration":0.010681,"end_time":"2023-07-03T18:48:13.57599","exception":false,"start_time":"2023-07-03T18:48:13.565309","status":"completed"},"tags":[]},"source":["#### (b) Fit a step function to predict wage using age, and perform crossvalidation to choose the optimal number of cuts. Make a plot of the fit obtained."]},{"cell_type":"code","execution_count":13,"id":"dfc66b14","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:13.600541Z","iopub.status.busy":"2023-07-03T18:48:13.599256Z","iopub.status.idle":"2023-07-03T18:48:13.608633Z","shell.execute_reply":"2023-07-03T18:48:13.607361Z"},"papermill":{"duration":0.023447,"end_time":"2023-07-03T18:48:13.61029","exception":false,"start_time":"2023-07-03T18:48:13.586843","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"b8c0213f","metadata":{"papermill":{"duration":0.010621,"end_time":"2023-07-03T18:48:13.631876","exception":false,"start_time":"2023-07-03T18:48:13.621255","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"ba9f4d05","metadata":{"papermill":{"duration":0.010549,"end_time":"2023-07-03T18:48:13.653081","exception":false,"start_time":"2023-07-03T18:48:13.642532","status":"completed"},"tags":[]},"source":["### 9. This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response.\n","#### (a) Use the poly() function to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.\n"]},{"cell_type":"code","execution_count":14,"id":"7646fc29","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:13.678092Z","iopub.status.busy":"2023-07-03T18:48:13.676666Z","iopub.status.idle":"2023-07-03T18:48:13.687554Z","shell.execute_reply":"2023-07-03T18:48:13.685952Z"},"papermill":{"duration":0.026616,"end_time":"2023-07-03T18:48:13.69047","exception":false,"start_time":"2023-07-03T18:48:13.663854","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"7286bd77","metadata":{"papermill":{"duration":0.010754,"end_time":"2023-07-03T18:48:13.712527","exception":false,"start_time":"2023-07-03T18:48:13.701773","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"78bc1a6f","metadata":{"papermill":{"duration":0.011117,"end_time":"2023-07-03T18:48:13.734561","exception":false,"start_time":"2023-07-03T18:48:13.723444","status":"completed"},"tags":[]},"source":["#### (b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares."]},{"cell_type":"code","execution_count":15,"id":"db887174","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:13.76219Z","iopub.status.busy":"2023-07-03T18:48:13.760495Z","iopub.status.idle":"2023-07-03T18:48:13.772684Z","shell.execute_reply":"2023-07-03T18:48:13.771121Z"},"papermill":{"duration":0.028264,"end_time":"2023-07-03T18:48:13.775171","exception":false,"start_time":"2023-07-03T18:48:13.746907","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"ddc2edfc","metadata":{"papermill":{"duration":0.011402,"end_time":"2023-07-03T18:48:13.798381","exception":false,"start_time":"2023-07-03T18:48:13.786979","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"c5347af1","metadata":{"papermill":{"duration":0.011235,"end_time":"2023-07-03T18:48:13.820935","exception":false,"start_time":"2023-07-03T18:48:13.8097","status":"completed"},"tags":[]},"source":["#### (c) Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results."]},{"cell_type":"code","execution_count":16,"id":"0ce7c892","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:13.846797Z","iopub.status.busy":"2023-07-03T18:48:13.845347Z","iopub.status.idle":"2023-07-03T18:48:13.856443Z","shell.execute_reply":"2023-07-03T18:48:13.854791Z"},"papermill":{"duration":0.026978,"end_time":"2023-07-03T18:48:13.85907","exception":false,"start_time":"2023-07-03T18:48:13.832092","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"f1c9bdbc","metadata":{"papermill":{"duration":0.011253,"end_time":"2023-07-03T18:48:13.881794","exception":false,"start_time":"2023-07-03T18:48:13.870541","status":"completed"},"tags":[]},"source":[]},{"cell_type":"markdown","id":"7621cb37","metadata":{"papermill":{"duration":0.011172,"end_time":"2023-07-03T18:48:13.904748","exception":false,"start_time":"2023-07-03T18:48:13.893576","status":"completed"},"tags":[]},"source":["#### (d) Use the bs() function to fit a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit."]},{"cell_type":"code","execution_count":17,"id":"3ab8dbb4","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:13.930214Z","iopub.status.busy":"2023-07-03T18:48:13.928866Z","iopub.status.idle":"2023-07-03T18:48:13.938407Z","shell.execute_reply":"2023-07-03T18:48:13.937152Z"},"papermill":{"duration":0.024717,"end_time":"2023-07-03T18:48:13.940516","exception":false,"start_time":"2023-07-03T18:48:13.915799","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"6a5f17c6","metadata":{"papermill":{"duration":0.011152,"end_time":"2023-07-03T18:48:13.962987","exception":false,"start_time":"2023-07-03T18:48:13.951835","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"a04f194c","metadata":{"papermill":{"duration":0.011324,"end_time":"2023-07-03T18:48:13.986146","exception":false,"start_time":"2023-07-03T18:48:13.974822","status":"completed"},"tags":[]},"source":["#### (e) Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained."]},{"cell_type":"code","execution_count":18,"id":"6b3c2b2b","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:14.011512Z","iopub.status.busy":"2023-07-03T18:48:14.010113Z","iopub.status.idle":"2023-07-03T18:48:14.020313Z","shell.execute_reply":"2023-07-03T18:48:14.018827Z"},"papermill":{"duration":0.02489,"end_time":"2023-07-03T18:48:14.022163","exception":false,"start_time":"2023-07-03T18:48:13.997273","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"404a1513","metadata":{"papermill":{"duration":0.011096,"end_time":"2023-07-03T18:48:14.045839","exception":false,"start_time":"2023-07-03T18:48:14.034743","status":"completed"},"tags":[]},"source":["# FIX ME"]},{"cell_type":"markdown","id":"e43c79d0","metadata":{"papermill":{"duration":0.010793,"end_time":"2023-07-03T18:48:14.067511","exception":false,"start_time":"2023-07-03T18:48:14.056718","status":"completed"},"tags":[]},"source":["#### (f) Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results."]},{"cell_type":"code","execution_count":19,"id":"53d991e3","metadata":{"execution":{"iopub.execute_input":"2023-07-03T18:48:14.113917Z","iopub.status.busy":"2023-07-03T18:48:14.112637Z","iopub.status.idle":"2023-07-03T18:48:14.121927Z","shell.execute_reply":"2023-07-03T18:48:14.12066Z"},"papermill":{"duration":0.045364,"end_time":"2023-07-03T18:48:14.123817","exception":false,"start_time":"2023-07-03T18:48:14.078453","status":"completed"},"tags":[]},"outputs":[],"source":["# FIX ME"]},{"cell_type":"markdown","id":"1da683c8","metadata":{"papermill":{"duration":0.01154,"end_time":"2023-07-03T18:48:14.146753","exception":false,"start_time":"2023-07-03T18:48:14.135213","status":"completed"},"tags":[]},"source":["# FIX ME"]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.0.5"},"papermill":{"default_parameters":{},"duration":6.473531,"end_time":"2023-07-03T18:48:14.277467","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-03T18:48:07.803936","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}